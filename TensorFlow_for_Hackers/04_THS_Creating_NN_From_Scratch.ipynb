{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"04_THS_Creating_NN_From_Scratch.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"z16hsBBPuaNk","colab_type":"text"},"source":["# [Creating a Neural Network from Scratch](https://medium.com/@curiousily/tensorflow-for-hackers-part-iv-neural-network-from-scratch-1a4f504dfa8)\n","\n","![](https://miro.medium.com/max/479/1*QVIyc5HnGDWTNX3m-nIm9w.png)\n","\n","This time we will skip TensorFlow entirely and build a Neural Network (shallow one) from scratch, using only pure Python and NumPy. The real challenge is to implement the core algorithm that is used to train (Deep) Neural Networks — Backpropagation.\n","\n","## Setup"]},{"cell_type":"code","metadata":{"id":"3uz91MS8t6CV","colab_type":"code","colab":{}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from pylab import rcParams\n","\n","from preprocessing import *\n","from math_utils import *\n","from plotting import *\n","\n","%matplotlib inline\n","\n","sns.set(style='whitegrid', palette='muted', font_scale=1.5)\n","\n","rcParams['figure.figsize'] = 12, 6\n","\n","RANDOM_SEED = 42\n","\n","np.random.seed(RANDOM_SEED)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WfSlSRCn7QvR","colab_type":"text"},"source":["# Background\n","\n","## Sigmoid (and it’s derivative)\n","\n","The sigmoid function is used quite commonly in the realm of deep learning, at least it was until recently. It has distinct S shape and it is a differentiable real function for any real input value. Additionally, it has a positive derivative at each point. More importantly, we will use it as an activation function for the hidden layer of our model. Here’s how it is defined:\n","\n","![](https://miro.medium.com/max/278/1*-PkQzu0E21YEbI9wvqvQzg.png)\n","\n","It’s first derivative (which we will use during the backpropagation step of our training algorithm) has the following formula:\n","\n","![](https://miro.medium.com/max/455/1*yT0ToBoL4o9eTgph6BWx4Q.png)\n","\n"]},{"cell_type":"code","metadata":{"id":"ootpoCTB5OWd","colab_type":"code","colab":{}},"source":["x = np.linspace(-10., 10., num=100)\n","sig = sigmoid(x)\n","sig_prime = sigmoid_prime(x)\n","\n","plt.plot(x, sig, label=\"sigmoid\")\n","plt.plot(x, sig_prime, label=\"sigmoid prime\")\n","plt.xlabel(\"x\")\n","plt.ylabel(\"y\")\n","plt.legend(prop={'size' : 16})\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hP-sqPnn7tXM","colab_type":"text"},"source":["![](https://miro.medium.com/max/730/1*Sek4P_MzBAipJJpwA8iS7Q.png)\n","\n","The derivative shows us the rate of change of a function. We can use it to determine the “slope” of that function. The highest rate of change for the sigmoid function is when x=0x=0, as it is evident from the derivative graph (in green).\n","\n","## Softmax\n","\n","The softmax function can be easily differentiated, it is pure (output depends only on input) and the elements of the resulting vector sum to 1. Here it is:\n","\n","![](https://miro.medium.com/max/573/1*gplkMOHmezbBphrtC0HbRQ.png)\n","\n","In probability theory, the output of the softmax function is sometimes used as a representation of a categorical distribution.\n","\n","The softmax function highlights the largest value(s) and suppresses the smaller ones.\n","\n","\n","## Backpropagation\n","\n","Backpropagation is the backbone of almost anything we do when using Neural Networks. The algorithm consists of 3 subtasks:\n","\n","- Make a forward pass\n","- Calculate the error\n","- Make backward pass (backpropagation)\n","\n","In the first step, backprop uses the data and the weights of the network to compute a prediction. Next, the error is computed based on the prediction and the provided labels. The final step propagates the error through the network, starting from the final layer. Thus, the weights get updated based on the error, little by little.\n","\n","We will try to create a Neural Network (NN) that can properly predict values from the XOR function. Here is its truth table:\n","![](https://miro.medium.com/max/1628/1*kmGgyHl3oj-iZOXPkfo0RA.png)\n","\n","Let start by defining some parameters:"]},{"cell_type":"code","metadata":{"id":"TNQIPUHU7shi","colab_type":"code","colab":{}},"source":["epochs = 50000\n","input_size, hidden_size, output_size = 2, 3, 1\n","LR = .1 # learning rate\n","\n","\n","\n","# Data\n","X = np.array([[0,0], [0,1], [1,0], [1,1]])\n","y = np.array([ [0],   [1],   [1],   [0]])\n","\n","\n","# Initialize the weights of our NN to random numbers (using proper size):\n","w_hidden = np.random.uniform(size=(input_size, hidden_size))\n","w_output = np.random.uniform(size=(hidden_size, output_size))\n","\n","\n","# implementation of the Backprop algorithm:\n","for epoch in range(epochs):\n"," \n","    # Forward\n","    act_hidden = sigmoid(np.dot(X, w_hidden))\n","    output = np.dot(act_hidden, w_output)\n","    \n","    # Calculate error\n","    error = y - output\n","    \n","    if epoch % 5000 == 0:\n","        print(f'error sum {sum(error)}')\n","\n","    # Backward\n","    dZ = error * LR\n","    w_output += act_hidden.T.dot(dZ)\n","    dH = dZ.dot(w_output.T) * sigmoid_prime(act_hidden)\n","    w_hidden += X.T.dot(dH)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ylrx8d809TA7","colab_type":"text"},"source":["That error seems to be decreasing! Yay! And the implementation is not that scary, isn’t it? We just multiply the matrix containing our training data with the matrix of the weights of the hidden layer. Then, we apply the activation function (sigmoid) to the result and multiply that with the weight matrix of the output layer.\n","\n","The error is computed by doing simple subtraction. During the backpropagation step, we adjust the weight matrices using the already computed error and use the derivative of the sigmoid function.\n","\n","Let’s try to predict using our trained model (doing just the forward step):"]},{"cell_type":"code","metadata":{"id":"eJnU0YoD5LPr","colab_type":"code","colab":{}},"source":["X_test = X[1] # [0, 1]\n","\n","act_hidden = sigmoid(np.dot(X_test, w_hidden))\n","np.dot(act_hidden, w_output)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Cfmcx3lb9cUD","colab_type":"text"},"source":["# Building our own Neural Network Classifier\n","\n","### Reading and shuffling the images"]},{"cell_type":"code","metadata":{"id":"WPUMA6fI9Y2q","colab_type":"code","colab":{}},"source":["IMAGES_PATH = 'train-images-idx3-ubyte'\n","LABELS_PATH = 'train-labels-idx1-ubyte'\n","\n","N_FEATURES = 28 * 28\n","N_CLASSES = 10"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"a5dJfyc-9ifD","colab_type":"code","colab":{}},"source":["X, y = read_mnist(IMAGES_PATH, LABELS_PATH)\n","X, y = shuffle_data(X, y, random_seed=RANDOM_SEED)\n","X_train, y_train = X[:500], y[:500]\n","X_test, y_test = X[500:], y[500:]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"swme37NN9vGd","colab_type":"text"},"source":["### Data exploration"]},{"cell_type":"code","metadata":{"id":"dndQgqyE9l59","colab_type":"code","colab":{}},"source":["plot_digit(X, y, idx=1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UzRFauEC9yr4","colab_type":"code","colab":{}},"source":["plot_digit(X, y, idx=3)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Bok9jX4c96kA","colab_type":"text"},"source":["## Implementing the model\n","Let’s define a class, called **NNClassifier** that does all the dirty work for us. We will implement a somewhat more sophisticated version of our training algorithm shown above along with some handy methods:"]},{"cell_type":"code","metadata":{"id":"5nsmWYsI93Eh","colab_type":"code","colab":{}},"source":["class NNClassifier:\n","\n","    def __init__(self, n_classes, n_features, n_hidden_units=30,\n","                 l1=0.0, l2=0.0, epochs=500, learning_rate=0.01,\n","                 n_batches=1, random_seed=None):\n","\n","        if random_seed:\n","            np.random.seed(random_seed)\n","        self.n_classes = n_classes\n","        self.n_features = n_features\n","        self.n_hidden_units = n_hidden_units\n","        self.w1, self.w2 = self._init_weights()\n","        self.l1 = l1\n","        self.l2 = l2\n","        self.epochs = epochs\n","        self.learning_rate = learning_rate\n","        self.n_batches = n_batches\n","\n","    def _init_weights(self):\n","        w1 = np.random.uniform(-1.0, 1.0, \n","                               size=self.n_hidden_units * (self.n_features + 1))\n","        w1 = w1.reshape(self.n_hidden_units, self.n_features + 1)\n","        w2 = np.random.uniform(-1.0, 1.0, \n","                               size=self.n_classes * (self.n_hidden_units + 1))\n","        w2 = w2.reshape(self.n_classes, self.n_hidden_units + 1)\n","        return w1, w2\n","\n","    def _add_bias_unit(self, X, how='column'):\n","        if how == 'column':\n","            X_new = np.ones((X.shape[0], X.shape[1] + 1))\n","            X_new[:, 1:] = X\n","        elif how == 'row':\n","            X_new = np.ones((X.shape[0] + 1, X.shape[1]))\n","            X_new[1:, :] = X\n","        return X_new\n","\n","    def _forward(self, X):\n","        net_input = self._add_bias_unit(X, how='column')\n","        net_hidden = self.w1.dot(net_input.T)\n","        act_hidden = sigmoid(net_hidden)\n","        act_hidden = self._add_bias_unit(act_hidden, how='row')\n","        net_out = self.w2.dot(act_hidden)\n","        act_out = sigmoid(net_out)\n","        return net_input, net_hidden, act_hidden, net_out, act_out\n","    \n","    def _backward(self, net_input, net_hidden, act_hidden, act_out, y):\n","        sigma3 = act_out - y\n","        net_hidden = self._add_bias_unit(net_hidden, how='row')\n","        sigma2 = self.w2.T.dot(sigma3) * sigmoid_prime(net_hidden)\n","        sigma2 = sigma2[1:, :]\n","        grad1 = sigma2.dot(net_input)\n","        grad2 = sigma3.dot(act_hidden.T)\n","        return grad1, grad2\n","\n","    def _error(self, y, output):\n","        L1_term = L1_reg(self.l1, self.w1, self.w2)\n","        L2_term = L2_reg(self.l2, self.w1, self.w2)\n","        error = cross_entropy(output, y) + L1_term + L2_term\n","        return 0.5 * np.mean(error)\n","\n","    def _backprop_step(self, X, y):\n","        net_input, net_hidden, act_hidden, net_out, act_out = self._forward(X)\n","        y = y.T\n","\n","        grad1, grad2 = self._backward(net_input, net_hidden, act_hidden, act_out, y)\n","\n","        # regularize\n","        grad1[:, 1:] += (self.w1[:, 1:] * (self.l1 + self.l2))\n","        grad2[:, 1:] += (self.w2[:, 1:] * (self.l1 + self.l2))\n","\n","        error = self._error(y, act_out)\n","        \n","        return error, grad1, grad2\n","\n","    def predict(self, X):\n","        Xt = X.copy()\n","        net_input, net_hidden, act_hidden, net_out, act_out = self._forward(Xt)\n","        return mle(net_out.T)\n","    \n","    def predict_proba(self, X):\n","        Xt = X.copy()\n","        net_input, net_hidden, act_hidden, net_out, act_out = self._forward(Xt)\n","        return softmax(act_out.T)\n","\n","    def fit(self, X, y):\n","        self.error_ = []\n","        X_data, y_data = X.copy(), y.copy()\n","        y_data_enc = one_hot(y_data, self.n_classes)\n","        for i in range(self.epochs):\n","\n","            X_mb = np.array_split(X_data, self.n_batches)\n","            y_mb = np.array_split(y_data_enc, self.n_batches)\n","            \n","            epoch_errors = []\n","\n","            for Xi, yi in zip(X_mb, y_mb):\n","                \n","                # update weights\n","                error, grad1, grad2 = self._backprop_step(Xi, yi)\n","                epoch_errors.append(error)\n","                self.w1 -= (self.learning_rate * grad1)\n","                self.w2 -= (self.learning_rate * grad2)\n","            self.error_.append(np.mean(epoch_errors))\n","        return self\n","    \n","    def score(self, X, y):\n","        y_hat = self.predict(X)\n","        return np.sum(y == y_hat, axis=0) / float(X.shape[0])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AKhA4bSf_SEs","colab_type":"text"},"source":["All the magic is hidden within the *_forward*, *_backward*, *_error* and *_backprop_step* methods. We measure the error using cross-entropy loss function. Additionally, L1 and L2 regularizations are used to drive our training into simpler models. One preprocessing step that our model is doing internally is the encoding of the labels as one-hot vectors via the helper function — *one_hot*.\n","Our NN has a neat interface, too! Use the *fit* method to train it, *predict* to predict the class of a digit and *score* to assess the overall performance of the model.\n","\n","## Training\n","\n","It’s time to reap the benefits of our hard work. Let’s train our NN for 300 epochs with 50 neurons in the hidden layer:"]},{"cell_type":"code","metadata":{"id":"Aapvn3cl_MLs","colab_type":"code","colab":{}},"source":["nn = NNClassifier(n_classes=N_CLASSES, \n","                  n_features=N_FEATURES,\n","                  n_hidden_units=50,\n","                  l2=0.5,\n","                  l1=0.0,\n","                  epochs=300,\n","                  learning_rate=0.001,\n","                  n_batches=25,\n","                  random_seed=RANDOM_SEED)\n","\n","nn.fit(X_train, y_train);"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fo5EPtZc_tqV","colab_type":"text"},"source":["## Evaluation\n","\n","First, let’s have a look at the error change as the number of training epochs increase:"]},{"cell_type":"code","metadata":{"id":"kYAUopJT_qve","colab_type":"code","colab":{}},"source":["plot_error(nn)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1oURIwP8_w1e","colab_type":"code","colab":{}},"source":["print('Train Accuracy: %.2f%%' % (nn.score(X_train, y_train) * 100))\n","print('Test Accuracy: %.2f%%' % (nn.score(X_test, y_test) * 100))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"crhMl8lh_0eq","colab_type":"code","colab":{}},"source":["nn.predict_proba(X_test[1:2])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fahuEU3OAARI","colab_type":"text"},"source":["## Correct prediction\n","\n","Let’s look at the image itself:\n"]},{"cell_type":"code","metadata":{"id":"8Od85P-__6F3","colab_type":"code","colab":{}},"source":["plot_digit(X_test, y_test, idx=1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"og-FA4QhAH-t","colab_type":"text"},"source":["And the probability distribution:"]},{"cell_type":"code","metadata":{"id":"Eh0OcueWAD8J","colab_type":"code","colab":{}},"source":["plot_digit_dist(X_test, y_test, idx=1, model=nn)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q0W0Tl95AN6j","colab_type":"text"},"source":["### Wrong prediction"]},{"cell_type":"code","metadata":{"id":"tr0DJ9vdAKR6","colab_type":"code","colab":{}},"source":["plot_digit(X_test, y_test, idx=70)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7Ox71tsEAREy","colab_type":"code","colab":{}},"source":["plot_digit_dist(X_test, y_test, idx=70, model=nn)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3CPOI0G7AZwF","colab_type":"text"},"source":["### MLE = picking the most probable digit\n","\n","Ok, but how does the prediction work? Simply put, it uses the most probable value in the class distribution:"]},{"cell_type":"code","metadata":{"id":"Fs8s9e5AAUGP","colab_type":"code","colab":{}},"source":["mle(nn.predict_proba(X_test[:5]))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Di73YljoAivQ","colab_type":"text"},"source":["If we use the predict method we obtain the same result"]},{"cell_type":"code","metadata":{"id":"FHW50iXkAdiP","colab_type":"code","colab":{}},"source":["nn.predict(X_test[:5])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7sXOY344Arbu","colab_type":"text"},"source":["## Trying a bit harder\n","\n","\n","The performance of our model was not that great. Can we improve on that?\n","\n","Let’s try to scale our input data:"]},{"cell_type":"code","metadata":{"id":"LGK1viBBAnjh","colab_type":"code","colab":{}},"source":["from sklearn.preprocessing import scale, normalize\n","\n","X_train_std = scale(X_train.astype(np.float64))\n","X_test_std = scale(X_test.astype(np.float64))\n","\n","nn = NNClassifier(n_classes=N_CLASSES, \n","                  n_features=N_FEATURES,\n","                  n_hidden_units=50,\n","                  l2=0.5,\n","                  l1=0.0,\n","                  epochs=300,\n","                  learning_rate=0.001,\n","                  n_batches=25,\n","                  random_seed=RANDOM_SEED)\n","\n","nn.fit(X_train_std, y_train);\n","\n","print('Test Accuracy: %.2f%%' % (nn.score(X_test_std, y_test) * 100))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nbrwDHlnA3N4","colab_type":"text"},"source":["Not bad, about 3% increase using simple preprocessing. What if we fiddle with the parameters a bit:"]},{"cell_type":"code","metadata":{"id":"6GTc43jwAy7_","colab_type":"code","colab":{}},"source":["nn = NNClassifier(n_classes=N_CLASSES, \n","                  n_features=N_FEATURES,\n","                  n_hidden_units=250,\n","                  l2=0.5,\n","                  l1=0.0,\n","                  epochs=500,\n","                  learning_rate=0.001,\n","                  n_batches=25,\n","                  random_seed=RANDOM_SEED)\n","\n","nn.fit(X_train, y_train);\n","\n","print('Test Accuracy: %.2f%%' % (nn.score(X_test, y_test) * 100))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3nY2QiVtA5hW","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}